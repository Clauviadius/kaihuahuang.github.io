---
layout:     post
title:      Regression Summary
subtitle:   From basic to lasso regression
date:       2018-11-25
author:     William
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Optimization
    - Gradient Descent
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: { 
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
  </script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
# Gradient Descent
## Introduction

Suppose $C(w)$ is the cost function and it's convex which means it's free of local minimum.
<center><img width="250" height="188" class="center-image" alt="ConcaveConvexFunction" src="images/eps-gif/ConcaveConvexFunction_1000.gif"></center>




