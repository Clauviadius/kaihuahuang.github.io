---
layout:     post
title:      Gradient Descent
subtitle:   Convex Optimization Basic
date:       2018-11-25
author:     William
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Optimization
    - Gradient Descent
---
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: { 
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
  </script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
# Gradient Descent
## Introduction

Suppose $C(w)$ is the cost function and it's convex which means it's free of local minimum.
<center>![](http://ww1.sinaimg.cn/large/83d6b255ly1fxxn51wx60j208005eq2t.jpg)</center>



